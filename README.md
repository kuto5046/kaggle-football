# Google Research Football with Manchester City F.C.

![](./img/001.png "football game display")
competition link is [here](https://www.kaggle.com/c/google-football)

---
## Contribution

### Competition
Final Score 59th(ğŸ¥‰)
![](./img/041.png)
  
  
### Notebook (ğŸ¥ˆ1ğŸ¥‰1)
![](./img/039.png "football game display")
  

### Discussion (ğŸ¥‡2ğŸ¥‰3)
![](./img/040.png "football game display")

---

## Rules
### åŸºæœ¬ãƒ«ãƒ¼ãƒ«
11äººãƒãƒ¼ãƒ ã®1äººã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æ“ä½œå¯èƒ½  
ãƒ«ãƒ¼ãƒ«ã¯å…¬å¼ã®ã‚µãƒƒã‚«ãƒ¼ã¨ã»ã¨ã‚“ã©åŒã˜(ã‚ªãƒ•ã‚µã‚¤ãƒ‰ã€ã‚¤ã‚¨ãƒ­ãƒ¼ã‚«ãƒ¼ãƒ‰ã€ãƒ¬ãƒƒãƒ‰ã‚«ãƒ¼ãƒ‰)  
ã—ã‹ã—åƒ…ã‹ãªãŒã‚‰é•ã„ãŒã‚ã‚‹

- ã‚²ãƒ¼ãƒ ã¯2ã¤ã®ãƒãƒ¼ãƒ•ã€45åˆ†ï¼ˆ1500 stepsï¼‰ãšã¤ã§æ§‹æˆã€‚(ã¤ã¾ã‚Š1step=1.8s)
å„ãƒãƒ¼ãƒ•ã®é–‹å§‹æ™‚ã®ã‚­ãƒƒã‚¯ã‚ªãƒ•ã¯åˆ¥ã®ãƒãƒ¼ãƒ ã§è¡Œã‚ã‚Œã‚‹ãŒã€ã‚µã‚¤ãƒ‰ã®å…¥ã‚Œæ›¿ãˆã¯ãªã—ï¼ˆã‚²ãƒ¼ãƒ ã¯å®Œå…¨ã«å·¦å³å¯¾ç§°ã§ã™ï¼‰ã€‚
- å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ãƒãƒ¼ãƒ å†…ã®1äººã®é¸æ‰‹ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã€‚ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã•ã‚ŒãŸãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€**å¸¸ã«ãƒœãƒ¼ãƒ«ã‚’æŒã£ã¦ã„ã‚‹ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€ã¾ãŸã¯å®ˆå‚™æ™‚ã«ãƒœãƒ¼ãƒ«ã«è¿‘ã„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ãªã‚‹ã€‚**
- ã‚²ãƒ¼ãƒ ä¸­ã«ãƒãƒ¼ãƒ ãŒã‚µã‚¤ãƒ‰ã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã“ã¨ã¯ãªã„ã€‚å·¦å³ã®ã‚µã‚¤ãƒ‰ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã€‚
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ã®ä¾¿å®œä¸Šï¼Œæä¾›ã•ã‚Œã‚‹ã‚ªãƒ–ã‚¶ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¯ï¼Œ**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå·¦ãƒãƒ¼ãƒ ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã—ã¦ã„ã‚‹ã‹ã®ã‚ˆã†ã«å¸¸ã«è¡¨ç¤ºã•ã‚Œã‚‹**ï¼
- ç’°å¢ƒã¯ã€è¦³å¯Ÿã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸¡æ–¹ã«é©åˆ‡ãªå¤‰æ›ã‚’é©ç”¨ã™ã‚‹ã€‚ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã¯å®Œå…¨ã«å¯¾ç§°çš„ã§ã‚ã‚‹ãŸã‚ã€å·¦å³ã®å…¥ã‚Œæ›¿ãˆã¯ã‚²ãƒ¼ãƒ ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã€‚
- éã‚«ãƒƒãƒ—ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã€ã‚ˆã‚Šå¤šãã®ã‚´ãƒ¼ãƒ«ã‚’æ±ºã‚ãŸãƒãƒ¼ãƒ ãŒå‹ã¡ã€‚
- ãƒãƒ¼ãƒ ã®é¸æ‰‹æ•°ãŒ7äººä»¥ä¸‹ã«ãªã£ãŸå ´åˆã€ã‚¦ã‚©ãƒ¼ã‚¯ã‚ªãƒ¼ãƒãƒ¼ã¯é©ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚
- äº¤ä»£é¸æ‰‹ã¯å­˜åœ¨ã—ãªã„ã€‚
- å»¶é•·æˆ¦ã¯é©ç”¨ã•ã‚Œãªã„ã€‚

### ã‚ªãƒ–ã‚¶ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨è¡Œå‹•
å„ã‚¿ãƒ¼ãƒ³ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç¾åœ¨ã®ã‚¹ã‚³ã‚¢ã€å…¨ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ä½ç½®ã€ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã€ç–²åŠ´åº¦ã‚’å«ã‚€ã‚²ãƒ¼ãƒ ã®å®Œå…¨ãªçŠ¶æ…‹ã‚’è¡¨ã™ã‚ªãƒ–ã‚¶ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å—ã‘å–ã‚‹ã€‚
ã‚ªãƒ–ã‚¶ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®è©³ç´°ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯[ã“ã¡ã‚‰](https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations)  
å„ã‚¿ãƒ¼ãƒ³ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã‹ã‚‰19ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ0ã‹ã‚‰18ã¾ã§ã®ç•ªå·ï¼‰ã®ã†ã¡ã®1ã¤ã‚’ç”Ÿæˆã™ã‚‹ã€‚
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã®å¤–ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿”ã™ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æå¤±ã¨ãªã‚‹ã€‚[(å‚è€ƒ)](https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#default-action-set)


### ã‚²ãƒ¼ãƒ çµ‚äº†
3000ã‚¿ãƒ¼ãƒ³å¾Œã€ã¾ãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã©ã¡ã‚‰ã‹ãŒã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã—ãŸå ´åˆ(ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€ä¾‹å¤–ãŒæŠ•ã’ã‚‰ã‚ŒãŸå ´åˆã€ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒè¿”ã•ã‚ŒãŸå ´åˆ)ã«ã‚²ãƒ¼ãƒ çµ‚äº†ã€‚
ã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè² ã‘ã€ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‹ã¤ã€‚ã‚¨ãƒ©ãƒ¼ãŒãªã‹ã£ãŸå ´åˆã¯ã€ã‚ˆã‚Šå¤šãã®ã‚´ãƒ¼ãƒ«ã‚’ç²å¾—ã—ãŸãƒãƒ¼ãƒ ãŒå‹ã¡ã€‚
ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¯è©•ä¾¡ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦æ›´æ–°ã€‚

---

## log

### [2020/09/30]
discussionã«GRFè«–æ–‡ã®å†…å®¹ã‚’æŠ•ç¨¿ã€€
https://www.kaggle.com/c/google-football/discussion/187657

[å…¬å¼ã‹ã‚‰ã®discussion](https://www.kaggle.com/c/google-football/discussion/187381)ã‚ˆã‚Š
- validation episodeãŒ75åˆ†ã¨ã¨ã¦ã‚‚é•·ã„ã“ã¨
- windowsä¸Šã§ã¯ç’°å¢ƒãŒã†ã¾ãå‹•ã‹ãªã„ã“ã¨
- ã”ããŸã¾ã«validation episodeãŒçµ‚äº†ã—ãªã„ã“ã¨ãŒã‚ã‚‹
- æ–°ã—ã„episodesãŒå‡¦ç†ã•ã‚Œãªã„

ã‚¢ã‚¤ãƒ‡ã‚¢ã€€CNNã«å…¥åŠ›ã™ã‚‹éš›ã«æ¬¡å…ƒæ•°ã‚’å¢—ã‚„ã—SMMã‚’pixelå…¥åŠ›ã«é‡ã­ã¦å…¥åŠ›

ã‚„ã£ãŸã“ã¨
- googlefootã®APIã‚’è§¦ã‚‹
- ï¼’ã¤ç›®ã®SEED RLã‚’subã—ã¦å¼·åŒ–å­¦ç¿’ã®æ§˜å­ã‚’è¦‹ã‚‹
- ãƒ•ã‚¡ã‚¤ãƒ«ã®outputå½¢å¼ãŒtzå½¢å¼


### [2020/10/01]
SEED RL[2]ã®è«–æ–‡ã‚’èª­ã‚“ã ã€‚
<div align="center"><img src="./img/002.png" width=400 title="SEED-RL Table 2"></div>

- TPUã«ã‚ˆã£ã¦è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ã‹ãªã‚ŠæŠ‘ãˆã‚‰ã‚Œã‚‹ä¸€æ–¹p100ã ã¨ã‚ã¾ã‚Šå‘ä¸Šã—ãªã„
- è¨ˆç®—åŠ¹ç‡ã¯é«˜ã„ã®ã§SEED RLã‚’ãƒ™ãƒ¼ã‚¹ã«å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„å ±é…¬ã‚’ã‚’å¤‰æ›´ã™ã‚‹æ–¹é‡ã§é€²ã‚ã‚‹
- TPU v3ã‚’32ã‚³ã‚¢ä½¿ã†ã¨max(1è©¦è¡Œï¼Ÿ)ãŒscoring rewardã§4.76(4ç‚¹ç›¸å½“), checkpointã§ãŒ7.66ã¨ã‹ãªã‚Šå¤§ãã„.
ã—ã‹ã—ä¸Šè¨˜ã¯è¨ˆç®—æ©ŸãŒãªã„ã®ã§æµçŸ³ã«å®Ÿç¾ä¸å¯(4å„„ãƒ•ãƒ¬ãƒ¼ãƒ å­¦ç¿’ã¨æ›¸ã„ã¦ã„ã‚‹ã€æ„å‘³ãŒã‚ã‹ã‚‰ãªã„)
- ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’æ¥µç«¯ã«å¤§ããå–ã‚‰ãªã„é™ã‚ŠSMM(super mini map)ã®image sizeã‚’å¤§ããã—ã¦ã‚‚å¤‰åŒ–ã¯ãªã„ã¿ãŸã„
- SEED RLã®SMM defaultã§$345(4ä¸‡ãã‚‰ã„?)

ãƒ–ãƒ­ã‚°è¨˜äº‹[4]ã‚’èª­ã‚“ã 
æœ¬ã‚²ãƒ¼ãƒ ç’°å¢ƒã¯ï¼“ç¨®é¡ã®è¡¨ç¾ãŒã‚ã‚‹
- Pixelsè¡¨ç¾ã¯è‡ªç„¶ã«è¦‹ãˆã‚‹ãŒã€ãƒã‚¤ã‚ºãŒå¤šãã€è¡¨ç¾ã‚µã‚¤ã‚ºãŒè†¨å¤§ãªãŸã‚ã«å¤±æ•—
- Super MiniMapã¨Simple115ã¯åŒç­‰ã®è¡¨ç¾ã§ã‚ã‚‹ãŒã€Simple115ã®æ–¹ãŒç’°å¢ƒã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„ã€‚
- è«–æ–‡ã§ã¯, Super MiniMap > Floats(Simple115)
- ã“ã®çµæœã‚’ç›´æ„Ÿçš„ã§ã¯ãªã„ã‚‚ã®ã¨è€ƒãˆ,"Floats "è¡¨ç¾ã‚’ä½¿ç”¨ã™ã‚‹å¼·åŠ›ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆã‚’è©¦ã¿ã‚‹ã€‚

<div align="center"><img src="./img/003.png" title="result PPO"></div>
easy modeã¯ã†ã¾ãã„ã£ã¦ã„ã‚‹ãŒ,hard modeã¯ãŠã‚‚ã‚ã—ããªã„ã€‚easy modeã®ã¿ã§ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã—ã¦ã—ã¾ã†(hard modeã«å¯¾å¿œã§ããªã„)  

ãã“ã§**å¹³å‡å ±é…¬ãŒé¸æŠã•ã‚ŒãŸã—ãã„å€¤ã«é”ã™ã‚‹ã¨ã€æ¬¡ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‹ã‚‰é›£æ˜“åº¦ã‚’ã‚¤ãƒ—ã‚·ãƒ­ãƒ³å¢—åŠ ã•ã›ã‚‹**
<div align="center"><img src="./img/004.png" width=500 title="result Îµ scheduling"></div>

- è‰¯ã„åŠ¹æœãŒå‡ºã¦ã„ã‚‹
- ãŸã ã—800Mã‚¹ãƒ†ãƒƒãƒ—ã«15æ—¥ã‹ã‹ã‚‹  
- è¨ˆç®—åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«32 CPUs(instead of 16), 64 parallel environments(instead of 16) 
- **CHECKPOINTå ±é…¬ã¯åˆã‚ã«é©ç”¨ã•ã›ã‚´ãƒ¼ãƒ«ãŒæ±ºã¾ã‚‹ã‚ˆã†ã«ãªã£ãŸã‚‰æ¸›è¡°ã•ã›0ã«è¿‘ã¥ã‘ã¦ã„ãã®ãŒãŠã™ã™ã‚**

### [2020/10/02]
- å­¦ç¿’ã«æ™‚é–“ã‚’è¦ã™ã‚‹ã®ã¨çµæœã®åˆ†æ•£ãŒå¤§ãã„ã“ã¨ã‹ã‚‰RLã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ä¸åˆ©ã§ã¯ã‚ã‚‹
- self play ã‚’è¡Œã†ãŸã‚ã«ã¯GRFã®ãƒ¬ãƒã‚¸ãƒˆãƒªå†…ã®example PPOã‚’å‚ç…§ã™ã‚‹
number_of_right_players_agent_controls=1ã«ã™ã‚‹

- æœªå­¦ç¿’ã®RL-agentã‚’è¦‹ã‚‹ã¨ãƒ•ã‚¡ãƒ¼ãƒ«ã‚„ã‚ªãƒ•ã‚µã‚¤ãƒ‰ãªã©ã®ãƒŸã‚¹ãŒå¤šã™ãã‚‹
- ã“ã‚Œã‚‰ã«ãƒã‚¤ãƒŠã‚¹å ±é…¬ã‚’ä¸ãˆã‚‹?ã—ã‹ã—ä»–ã®å®Ÿè£…ã§ã¯å ±é…¬ã¯CHECKPOINTã¨SCORINGã®ã¿ãªã®ã§ã‚ã¾ã‚Šæœ‰åŠ¹ã§ã¯ãªã„ï¼Ÿ
- å‰åŠã ã‘ä¸ãˆã‚‹ï¼Ÿ

æ–¹é‡
- easyã§å‹ã¦ã‚‹ãã‚‰ã„ã®ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹agentã‚’ä½œæˆã™ã‚‹
- ãã®éš›ã¯CHECKPOINTã¨FOULå ±é…¬ã‚’ä¸ãˆã‚‹

ã‚„ã£ãŸã“ã¨
- SEED RLã®flameæ•°ã‚’5Mã«è¨­å®šã—CPU, GPU, TPUã§å®Ÿè¡Œæ™‚é–“ã‚’æ¯”è¼ƒã™ã‚‹
- ã¡ãªã¿ã«è«–æ–‡å®Ÿè£…ã¯500M(4000M)
- å­¦ç¿’çµæœã‚’ä¿å­˜ã—9hã§å¼•ãç¶šãå­¦ç¿’ã™ã‚‹

- ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ã®å­¦ç¿’ã¨è‡ªå·±å­¦ç¿’ã¯ã©ã¡ã‚‰ãŒã‚ˆã„?
self-learning
äº‹å‰ã«å¯¾æˆ¦ç›¸æ‰‹ã®æˆ¦ç•¥ã«é–¢ã™ã‚‹æƒ…å ±ãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å¿…è¦ã¯ãªãï¼Œã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ãŒå¯èƒ½ã§ã‚ã‚‹  
ã¾ãŸï¼Œå¾ã€…ã«å¼·ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç›¸æ‰‹ã«å­¦ç¿’ã™ã‚‹ã‚ˆã†ã«ãªã‚‹ãŸã‚ï¼Œæœ€åˆã‹ã‚‰å¼·ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç›¸æ‰‹ã«ã™ã‚‹ã‚ˆã‚Šã‚‚å­¦ç¿’ãŒã‚¹ãƒ ãƒ¼ã‚ºã«é€²ã¿ã‚„ã™ã„


### [2020/10/03]
CPU times: Wall time: 3h 57min 48s  Total 5.7h


æ”»æ’ƒæ™‚ã¨å®ˆå‚™æ™‚ã§æ–¹ç­–ã‚’å¤‰ãˆã‚‹
episodeã¯ä½•å˜ä½ï¼Ÿ

### [2020/10/03]  
- PFRLã‚’ä½¿ã£ã¦rainbow DQNã‚’å®Ÿè£…ä¸­
- å…¥åŠ›ã®shapeãŒåˆã£ã¦ãŠã‚‰ãšerror
- ã¾ãšã¯baselineã‚’ä½œã‚ŠãŸã„
- $2000ã®GCPã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã¯è²°ãˆãªã‹ã£ãŸãŒæ¥é€±100åã«$1000ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’é…å¸ƒã™ã‚‹
- PFRLã§notebookã‚’ä»Šé€±ä¸­ã«æŠ•ç¨¿ã—ãŸã„
- å†…å®¹ã¯rainbow-DQNã‚’PFRLã§å®Ÿè£…ã—self-playã§è¨“ç·´ã—ã¦é‡ã¿ã‚’ä¿å­˜å¾Œsubmitã¾ã§


### [2020/10/04]  
- PFRLã®å®Ÿè£…ä¾‹ã‚’æ´—ã†
- åŸºæœ¬éƒ¨åˆ†env, NN, agentã¯ä»Šæ—¥ä¸­ã«ä½œæˆ 
- Self-playã®æ–¹æ³•ã«ã¤ã„ã¦å­¦ã¶

### [2020/10/05]  
ã‚¢ã‚¤ãƒ‡ã‚¢  
- æ–°ãŸãªå ±é…¬ã‚’åŠ ãˆã‚‹
å€™è£œ1. ãƒœãƒ¼ãƒ«æ”¯é…ç‡
å€™è£œ2. ãƒ•ã‚¡ãƒ¼ãƒ«, ã‚ªãƒ•ã‚µã‚¤ãƒ‰ã«ã¯ãƒã‚¤ãƒŠã‚¹å ±é…¬

- rewardé–¢æ•°ã®å ´æ‰€
football/gfootball/env/wrappers.py
SingleAgentRewardWrapper
CheckpointRewardWrapper

checkpointã®æ¦‚è¦

ä»¥ä¸‹ã®æ¡ä»¶ä»¥å¤–ã¯rewardã‚’å¾—ã‚‹
- ball_own_teamã¨ã„ã†keyãŒè¦³æ¸¬çŠ¶æ…‹ä¸­ã«ãªã„
- è¦³æ¸¬ã•ã‚ŒãŸçŠ¶æ…‹ã®ball_own_teamï¼0
- ball_own_playerã¨ã„ã†keyãŒè¦³æ¸¬çŠ¶æ…‹ã«ãªã„
- è¦³æ¸¬ã•ã‚ŒãŸçŠ¶æ…‹ã®ball_own_playerãŒactiveã§ãªã„

- Tensorflow kerasã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã å¾Œsummary()ãŒè¦‹ã‚Œãªã„
AttributeError: '_UserObject' object has no attribute 'summary'

TPU
CPU times: user 10.7 s, sys: 2.14 s, total: 12.9 s
Wall time: 6min

CPU
CPU times: user 7.34 s, sys: 1.08 s, total: 8.42 s
Wall time: 5min 13s
è¨ˆç®—æ™‚é–“å¤‰ã‚ã‚‰ãªã„ï¼Ÿãªãœï¼Ÿ

### [2020/10/05]  
PFRLã§rainbowã‚’å®Ÿè£…
ã¨ã‚Šã‚ãˆãštrainã¾ã§é€²ã‚ã‚‹ã“ã¨ãŒã§ããŸ
Wrapperã‚’ä½¿ã†ã“ã¨ã§envã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†
450000 flameã§å­¦ç¿’â†’9hã‚’è¶…ãˆã¦ã—ã¾ã£ãŸ

### [2020/10/06]  
è©•ä¾¡ã®ãƒ‘ãƒ¼ãƒˆã®ç†è§£ã¨çµæœã®å¯è¦–åŒ–ã‚’è¡Œã„ãŸã„
å ±é…¬ã®æ¨ç§»ã‚°ãƒ©ãƒ•
GPUã®ä½¿ã„æ–¹
å ±é…¬è¿½åŠ 

Qã¨lossãŒNanã«ãªã£ã¦ã„ã‚‹ã®ãŒæ°—ã«ãªã‚‹

### [2020/10/07]  
Rainbow-PFRLã§GPUã‚’ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ãŸ
ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒã†ã¾ãã„ã£ã¦ã„ã‚‹ã‹ãŒä¸æ˜
(loadå¾Œã®é‡ã¿ã«å¤‰åŒ–ãŒãªã„)

ã‚¢ã‚¤ãƒ‡ã‚¢
- Floatã¨SMMã‚’çµ„ã¿åˆã‚ã›ãŸæƒ…å ±ã§å­¦ç¿’
- Pixelã‚’ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’

### [2020/10/07]    
- Rainbowã®n_atomsã¯åˆ†å¸ƒå¼·åŒ–å­¦ç¿’ã®å…ƒè«–æ–‡ã‚’èª­ã‚€ã“ã¨ã§è§£æ±º
Rainbow 9000 step(CPU)

[CPU]
CPU times: user 11min 21s, sys: 25.7 s, total: 11min 46s
Wall time: 5min 53s

[GPU]
CPU times: user 4min 12s, sys: 854 ms, total: 4min 13s
Wall time: 4min 15s

å¤šå°‘ã®ãƒ–ãƒ¬(+-1 min)ã¯ã‚ã‚‹ã«ã—ã¦ã‚‚GPUã®æ–¹ãŒæ—©ãã†

Errorã«æ‚©ã¾ã•ã‚Œã¦ã„ã‚‹
Envã«wrapperã‚’ä½¿ã£ã¦obsã®å¤‰æ›ã‚’è¡Œãªã£ã¦ã„ã‚‹ã‚“ã ã‘ã©
å¤‰æ›ãŒã†ã¾ãã„ã‹ãªã„
ç”»åƒã‚µã‚¤ã‚ºã¨ã‹ã¯å¤‰ãˆãšã«shapeã®ã¿PyTorchã«åˆã†ã‚ˆã†ã«å¤‰æ›ã™ã‚‹


### [2020/10/12]   
`create_environment`ã‚’è¦‹ãŸ
Custom rewardã¯create_environmentã‹ã‚‰ã§ã¯ãªãã€æ–°ã—ã„wrapperé–¢æ•°ã‚’ä½œã‚‹ã ã‘ã§è‰¯ã„
rainbowã‚’1M step GPUã§å›ã™

### [2020/10/13]   
Failed. Exited with code 137.
Replay buffãŒå¤§ãã™ãã‚‹ã“ã¨ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã¨æ€ã‚ã‚Œã‚‹
CPUã®å ´åˆã¯315000 step(8.96h)
GPUã®å ´åˆã¯393000 step(3.6h)

Replay buffe 10 **6 â†’ 10**5ã«å¤‰æ›´

### [2020/10/14]   
Replay bufferã‚’æ¸›ã‚‰ã—ãŸã“ã¨ã§ãƒªãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯æ¶ˆãˆãŸ
Agentã®returnã«numpyã‚’ä½¿ã£ã¦ã„ãŸã“ã¨ãŒåŸå› ã§env.runãŒã†ã¾ãã„ã£ã¦ã„ãªã‹ã£ãŸã“ã‚Œã‚’è§£æ±º
TimeouterrorãŒèµ·ãã‚‹ã“ã¨ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼
â†’DeadlineExceeded()
ãŠãã‚‰ãè¡Œå‹•é¸æŠã«æ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã£ã¦ã„ã‚‹ã“ã¨ã«å¯¾ã™ã‚‹ã‚¨ãƒ©ãƒ¼
[GFootball: Rainbow-DQN [PFRL]](https://www.kaggle.com/kuto0633/gfootball-rainbow-dqn-pfrl)ã‚’å…¬é–‹

### [2020/10/15]  
pfrlãŒagenté–¢æ•°ãªã„ã§èª­ã¿è¾¼ã‚ã¦ã„ãªã„ã®ãŒåŸå› ã§subãŒã†ã¾ãã„ã‹ãªã„
<div align="center"><img src="./img/005.png" width=500 title="result Îµ scheduling"></div>
ã“ã‚ŒãŒç¢ºã‹1M stepã§å­¦ç¿’ã—ãŸã‚‚ã®
æ›´æ–°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§lossãŒä¸‹ãŒã‚‹ã®ã§ã“ã®ã‚ˆã†ã«è¦å‰‡æ­£ã—ã„ä¸‹ãŒã‚Šæ–¹ã‚’ã—ã¦ã„ã‚‹ï¼Ÿ
rewardã¯ã‚ã¾ã‚Šä¸ŠãŒã‚‰ãªã„(æ•´æ•°ã—ã‹æ’®ã£ã¦ã„ãªã„ã®ã ãŒcheckpointã¯ä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼Ÿ)


`stickytape`ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã‚µãƒ–ãƒŸãƒƒãƒˆå•é¡Œã‚’è§£æ±º
ã—ã‹ã—pipã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸpfrlãŒæ¶ˆã›ãªãã¦å›°ã£ã¦ã„ã‚‹

[2020/10/17]
create_envirnmentã‚’ç¢ºèªã—ãŸã¨ã“ã‚
```
if 'checkpoints' in rewards.split(','):
    env = wrappers.CheckpointRewardWrapper(env)
```
ç’°å¢ƒã‚’ä½œæˆã™ã‚‹éš›ã«
rewards = scoring, checkpoints,
ã‚«ãƒ³ãƒã®é–“ã«ç©ºç™½ãŒã‚ã‚‹ã¨ãƒ€ãƒ¡  
æ­£ã—ãã¯  `rewards = scoring,checkpoints`  
ä¸Šè¨˜å†…å®¹ã‚’discussionã«æŠ•ç¨¿
  
  
å ±é…¬ã®æ¸›è¡°ã‚’ã©ã†ã™ã‚‹ã‹å•é¡Œ
<div align="center"><img src="./img/006.png" width=500 title="result Îµ scheduling"></div>

```
gamma = 0.999999
num_steps = 10000000  # 10M steps
reward = 1
```

<div align="center"><img src="./img/007.png" width=500 title="result Îµ scheduling"></div>

```
gamma = 0.999
num_episodes = 3000
reward = 1
```

### [2020/10/18]

```
could not find proposed file
```
ãŠãã‚‰ãä¿å­˜å ´æ‰€ã®å•é¡Œ  
`/kaggle/working/` ç›´ä¸‹ã«pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç½®ã„ãŸã‚‰è§£æ±º  

runtime error
pfrlã®moduleã®èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚‹
60sä¸ãˆã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿ
```
[[{"duration": 10.403563, "stdout": "", "stderr": ""}]]
```

modelä½œæˆãªã—
weightãªã—
ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹actionã®å‡ºåŠ›ã‚‚ãªã—
ã§ã„ã‘ã‚‹


### [2020/10/19]
weightã‚’åœ§ç¸®ã›ãšã«
åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãŠã„ã¦tar.gzåœ§ç¸® or ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãªã—ã§submission.pyã«ã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹
```
[[{"duration": 0.002522, "stdout": "", "stderr": "Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/kaggle_environments/agent.py\", line 43, in get_last_callable\n    code_object = compile(raw, \"<string>\", \"exec\")\n  File \"<string>\", line 1\n    /kaggle_simulations/agent/main.py\n    ^\nSyntaxError: invalid syntax\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/kaggle_environments/agent.py\", line 159, in act\n    action = self.agent(*args)\n  File \"/opt/conda/lib/python3.7/site-packages/kaggle_environments/agent.py\", line 116, in callable_agent\n    agent = get_last_callable(raw) or raw\n  File \"/opt/conda/lib/python3.7/site-packages/kaggle_environments/agent.py\", line 58, in get_last_callable\n    raise InvalidArgument(\"Invalid raw Python: \" + repr(e))\nkaggle_environments.errors.InvalidArgument: Invalid raw Python: SyntaxError('invalid syntax', ('<string>', 1, 1, '/kaggle_simulations/agent/main.py\\n'))\n"}]]
```
â†‘â†“pathé€šã—ã¦ã“ã®ã‚¨ãƒ©ãƒ¼ã¯å›é¿ã—ãŸ

```
[
```
pfrlã‚’èª­ã¿è¾¼ã‚ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
```
[[{"duration": 9.90701, "stdout": "{'n_actions': 19, 'n_input_channels': 4, 'activation': <built-in method relu of type object at 0x7f01996d0000>, 'n_atoms': 51, 'training': True, '_parameters': OrderedDict(), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_modules': OrderedDict([('conv_layers', ModuleList(\n  (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n  (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n)), ('main_stream', FactorizedNoisyLinear(\n  (mu): Linear(in_features=3136, out_features=1024, bias=True)\n  (sigma): Linear(in_features=3136, out_features=1024, bias=True)\n)), ('a_stream', FactorizedNoisyLinear(\n  (mu): Linear(in_features=512, out_features=969, bias=True)\n  (sigma): Linear(in_features=512, out_features=969, bias=True)\n)), ('v_stream', FactorizedNoisyLinear(", "stderr": ""}]]
```
modelã«actionã‚’æ¸¡ã›ã¦ã¯ã„ã‚‹ãŒæ™‚é–“ã®å•é¡Œï¼Ÿ

modelã‹ã‚‰actionãªã—ã§ã‚ã‚Œã°ã„ã‘ã‚‹ã€€ãªãœ
åŸå› ã¨ãªã£ã¦ã„ã‚‹ã®ã¯`action = model(obs)`
åˆã‚ã®ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§
agenté–¢æ•°ã®å¤–ã§ä¸€åº¦ãƒ€ãƒŸãƒ¼ã®obsã‚’modelã«å…¥åŠ›ã—ã¦ãŠã
ä¸Šè¨˜ã§ã‚‚ã‚¨ãƒ©ãƒ¼

seedrlã®å–ã‚Šçµ„ã¿ã‚’çœŸä¼¼ã—ã¦ã¿ã‚‹
ã‚ã¾ã‚ŠçœŸä¼¼ã™ã‚‹ã¨ã“ã‚ãŒãªã„
```
[[{"duration": 10.403563, "stdout": "", "stderr": ""}]]
```

SEED RL Default TPU v3 2coreã§ã‚„ã£ã¦ã¿ã‚‹
1sã‚ãŸã‚Š18K stepsã¯æ—©ã™ãã‚‹
easyã‹ã‚‰difficultã«å¾ã€…ã«å¤‰æ›´

3ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§è¡Œã†
- Rainbow
- PPO2
- seedRL(v-trace)

é›¢æ•£çš„ã«é›£æ˜“åº¦ã‚’å¤‰ãˆã¦ãã®å¾Œã«ã§ããŸã‚‰å‹•çš„ã«ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å¤‰ãˆã‚‹

### [2020/10/20]  
é–¾å€¤ã‚’æ±ºã‚ã¦ãã®å€¤ã‚’3è©¦åˆåˆ†ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒè¶…ãˆãŸã‚‰é›£æ˜“åº¦ã‚’ä¸Šã’ã‚‹
football/gfootball/scenarios/11_vs_11_easy_stochastic.pyã®ä¸­èº«
```python
def build_scenario(builder):
  builder.config().game_duration = 3000
  builder.config().right_team_difficulty = 0.05
```
right_team_difficultyã‚’å‹•çš„ã«å¤‰ãˆãŸã„  
stepã§å¤‰ãˆã‚‹ã‹scoreã§å¤‰ãˆã‚‹ã‹  
ã©ã“ã§å¤‰ãˆã‚Œã°è‰¯ã„ï¼Ÿ  

`football/gfootball/env/config.py`  
ã“ã“ã«new scenarioã¨ã„ã†é–¢æ•°ãŒã‚ã‚‹ä½¿ãˆãã†  
configãŒã©ã“ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’èª¿ã¹ã‚‹ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’èª¿ã¹ã‚‹

`football/gfootball/env/scenario_builder.py`  
ã“ã“ã®Scenarioã‚¯ãƒ©ã‚¹ã§configãŒä½¿ã‚ã‚Œã¦ã„ã‚‹

ã²ã¨ã¾ãšä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ç’°å¢ƒã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã§æ‰‹å‹•ã§ã¯ã‚ã‚‹ãŒé›£æ˜“åº¦ã®å¤‰æ›´ãŒå¯èƒ½
å¯èƒ½ã§ã‚ã‚Œã°ã‚³ãƒ¼ãƒ‰ã§å¤‰æ›´ã—ãŸã„ãŒæš«å®šçš„ã«ã¯ã“ã‚Œã§é›£æ˜“åº¦èª¿æ•´ã‚’è¡Œã†

```python
%%writefile football/gfootball/scenarios/11_vs_11_custom_stochastic.py 
from . import *

def build_scenario(builder):
  builder.config().game_duration = 3000
  builder.config().right_team_difficulty = 0.2  # <-é›£æ˜“åº¦ã‚’å¤‰æ›´
  builder.config().deterministic = False
  if builder.EpisodeNumber() % 2 == 0:
    first_team = Team.e_Left
    second_team = Team.e_Right
  else:
    first_team = Team.e_Right
    second_team = Team.e_Left
  builder.SetTeam(first_team)
  builder.AddPlayer(-1.000000, 0.000000, e_PlayerRole_GK)
  builder.AddPlayer(0.000000,  0.020000, e_PlayerRole_RM)
  builder.AddPlayer(0.000000, -0.020000, e_PlayerRole_CF)
  builder.AddPlayer(-0.422000, -0.19576, e_PlayerRole_LB)
  builder.AddPlayer(-0.500000, -0.06356, e_PlayerRole_CB)
  builder.AddPlayer(-0.500000, 0.063559, e_PlayerRole_CB)
  builder.AddPlayer(-0.422000, 0.195760, e_PlayerRole_RB)
  builder.AddPlayer(-0.184212, -0.10568, e_PlayerRole_CM)
  builder.AddPlayer(-0.267574, 0.000000, e_PlayerRole_CM)
  builder.AddPlayer(-0.184212, 0.105680, e_PlayerRole_CM)
  builder.AddPlayer(-0.010000, -0.21610, e_PlayerRole_LM)
  builder.SetTeam(second_team)
  builder.AddPlayer(-1.000000, 0.000000, e_PlayerRole_GK)
  builder.AddPlayer(-0.050000, 0.000000, e_PlayerRole_RM)
  builder.AddPlayer(-0.010000, 0.216102, e_PlayerRole_CF)
  builder.AddPlayer(-0.422000, -0.19576, e_PlayerRole_LB)
  builder.AddPlayer(-0.500000, -0.06356, e_PlayerRole_CB)
  builder.AddPlayer(-0.500000, 0.063559, e_PlayerRole_CB)
  builder.AddPlayer(-0.422000, 0.195760, e_PlayerRole_RB)
  builder.AddPlayer(-0.184212, -0.10568, e_PlayerRole_CM)
  builder.AddPlayer(-0.267574, 0.000000, e_PlayerRole_CM)
  builder.AddPlayer(-0.184212, 0.105680, e_PlayerRole_CM)
  builder.AddPlayer(-0.010000, -0.21610, e_PlayerRole_LM)

```

east modeã§1M stepså­¦ç¿’ã—ãŸçµæœ  
è‹¥å¹²å³è‚©ä¸ŠãŒã‚Š
<div align="center"><img src="./img/008.png" width=500 title="result Îµ scheduling"></div>

### [2020/10/21]

custom_scenarioã‚’ã‚„ã‚‹ã¨ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼
ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã„ã˜ã‚‰ãªã„ã¨ãƒ€ãƒ¡ã½ã„
```
ERROR:absl:Loading scenario "11_vs_11_custom_stochastic" failed
ERROR:absl:No module named 'gfootball.scenarios.11_vs_11_custom_stochastic'
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/gfootball/env/scenario_builder.py in __init__(self, config)
     56     try:
---> 57       scenario = importlib.import_module('gfootball.scenarios.{}'.format(config['level']))
     58     except ImportError as e:

9 frames
ModuleNotFoundError: No module named 'gfootball.scenarios.11_vs_11_custom_stochastic'

During handling of the above exception, another exception occurred:

NameError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/gfootball/env/scenario_builder.py in __init__(self, config)
     59       logging.error('Loading scenario "%s" failed' % config['level'])
     60       logging.error(e)
---> 61       exit(1)
     62     scenario.build_scenario(self)
     63     self.SetTeam(libgame.e_Team.e_Left)

NameError: name 'exit' is not defined
```
ã²ã¨ã¾ãšæ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã™ã‚‹å½¢ã§å¯¾å‡¦ã™ã‚‹
é›£æ˜“åº¦=0.2ã§1M~2M stepsã‚’è¡Œã†

é›£æ˜“åº¦ã‚’å¤‰æ›´ã™ã‚‹ã®ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ã§æ§˜ã€…ãªæ•µã¨å¯¾æˆ¦ã™ã‚‹ã“ã¨ã«ãªã‚Šæ±åŒ–æ€§èƒ½ãŒä¸ŠãŒã‚Šãã†
æ˜“â†’é›£ã«ã™ã‚‹ã“ã¨ã§å­¦ç¿’ãŒé€²ã¿ã‚„ã™ãã†(self-playã‚„GANã®è€ƒãˆã«è¿‘ã„)

sheme
1M stepã”ã¨ã«é›£æ˜“åº¦ã‚’0.1ãšã¤ä¸Šã’ã‚‹  
0.5, 2, 3, ...... 8, 9.5  
difficult(0.95)ã¯2M steps  
ã“ã‚Œã‚’ï¼‘ã‚»ãƒƒãƒˆã¨ã—ã¦å­¦ç¿’  

difficulty=0.2ã®çµæœ(700Kstepsãã‚‰ã„ã§è½ã¡ãŸ)
<div align="center"><img src="./img/009.png" width=500 title="result Îµ scheduling"></div>

checkpointrewardã‚’ç°¡æ˜“çš„ã«è¿½åŠ ã§ããªã„ã‹ã¨æ€ã£ã¦è©¦è¡ŒéŒ¯èª¤ã—ã¦ã„ã‚‹  
wrapperã‚’customã—ã¦å¾Œä»˜ã‘ã—ãŸã„ãŒã§ãã¦ã„ãªã„    
  
ã‚„ã£ãŸã“ã¨  
checkpointsã¯ãªã—ã§env create  
checkpoint wrapperãªã„ã®stepã‚’ä¿®æ­£ã—ãŸãŒunwrappedã—ãŸã¨ãã«ãã®ä»–ã®wrapperã‚‚å‰Šé™¤ã•ã‚Œã¦ã—ã¾ã„ã†ã¾ãã„ã‹ãªã„  
ãŸã ã—stepsæ•°ã‚„configã¯å–ã‚Œã‚‹ã‹ã‚‚  
env.unwrapped._env. ~step?

ã‚µãƒ–è§£æ±ºã—ãŸã‹ã‚‚ï¼

```
action = actions.greedy_actions.numpy())
```
ä¸Šã‚’ä¸‹ã«å¤‰æ›´ã™ã‚‹
```
action = int(actions.greedy_actions.numpy()[0])
```
agenté–¢æ•°å†…ã§actionã‚’è¿”ã™ã¨ãã«tensorâ†’numpyâ†’listã¨ã—ã¦ã„ãŸ  
ãã®ã¨ãactionã«ã¯np.asarray([1])ãŒå…¥ã£ã¦ã„ã¦
list(action)ã¨ã—ã¦ã„ãŸã®ã§[[1]]ã¨ãªã£ã¦ã„ãŸï¼Ÿ  
printã®æ¨™æº–å‡ºåŠ›ã§ã¯[0]ã®ã‚ˆã†ã«å•é¡Œãªãå‡ºåŠ›ã•ã‚Œã¦ã„ãŸ

ä»¥ä¸‹ã¯2M stepsã®rainbowã®çµæœ  
0-1M steps (difficulty=0.05(easy))
<div align="center"><img src="./img/010.png" width=500 title="result Îµ scheduling"></div>
1-2M steps (difficulty=0.2)
<div align="center"><img src="./img/011.png" width=500 title="result Îµ scheduling"></div>
  
Rainbowä¿®æ­£ç‰ˆã‚’notebookã«å…¬é–‹  
[GFootball: Modified-Rainbow-DQN [PFRL]](https://www.kaggle.com/kuto0633/gfootball-modified-rainbow-dqn-pfrl)


### [2020/10/22]  
GCPã§seedrlã‚’å›ã™æº–å‚™
<div align="center"><img src="./img/012.png" title="result Îµ scheduling"></div>
<div align="center"><img src="./img/013.png" title="result Îµ scheduling"></div>

**ç’°å¢ƒè¨­å®š**  
- TPU 2cores
- CPU 416cores
- Batch 128
- SMM size default

**æ™‚é–“**
- 1s - 18K steps  
- 1h - 60M steps(2K episodes)

**ã‚³ã‚¹ãƒˆ**  
- $0.0475 * 416 = $20/h  
- $1.0 * 2 = $2/h  
- total 1hã‚ãŸã‚Š$22

ã²ã¨ã¾ãš1h(60M steps)ã§è©¦ã—ã¦ã¿ã‚‹

## [2020/10/22]
ãƒ–ãƒ­ã‚°è¨˜äº‹ã«å¾“ã„easy modeã‹ã‚‰è¨˜è¼‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§adaptive trainingã‚’60M stepsè¡Œã†
ãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã‚’å‚ç…§ã™ã‚‹ã¨ãŠãã‚‰ãscore 1ãã‚‰ã„ã§çµ‚äº†ã™ã‚‹ã®ã§ã¯ï¼Ÿ  
ãã®æ™‚ã«adaptive difficultyãŒã©ã®å€¤ã‹ã‚‚è¨˜éŒ²ã™ã‚‹
<div align="center"><img src="./img/004.png" width=500 title="result Îµ scheduling"></div>

logã¨ã—ã¦æ¬²ã—ã„æƒ…å ±ã®æ•´ç†
- å ±é…¬æ¨ç§»
- adaptive difficultyã®æ¨ç§»
- lossã®æ¨ç§»

---
**AI platform TPU 8coreã—ã‹ä½¿ãˆãªã„æ¨¡æ§˜**

**ç’°å¢ƒè¨­å®š**  
- TPU 8cores
- CPU 1664cores
- Batch 160
- SMM size default

**æ™‚é–“**
- 1s - 71K steps  
- 1h - 255M steps(2K episodes)

**ã‚³ã‚¹ãƒˆ**  
- $0.0475 * 1664 = $80/h  
- $1.0 * 8 = $8/h  
- total 1hã‚ãŸã‚Š$88(ç´„10,000å††)

### ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£  
<!-- 60M stepsã§ç´„15åˆ†â†ã“ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§å®Ÿæ–½   -->
1.adaptive-diff + checkouts  
2.adaptive_diff + checkouts + penalty_rewards  
3.adaptive_diff + checkouts + smm_large  

---

### AI platform  
[ã“ã“](https://github.com/google-research/seed_rl)ã®è¨˜è¿°ã«å¾“ã† 
1. cloud-sdkã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«  
2. è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è«‹æ±‚ãŒå¯èƒ½ã‹ç¢ºèª
3. Cloud Machine Learning Engineã¨Compute Engine APIs.ã‚’æœ‰åŠ¹åŒ–
4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯(bucketã®ç™»éŒ²ã¯docker buildæ™‚ã«è‡ªå‹•ã§ä½œã‚‰ã‚Œã‚‹ã®ã§ã—ãªãã¦ã‚‚è‰¯ã„)
https://cloud.google.com/ml-engine/docs/working-with-cloud-storage.

ä»®ã«bucketã‚’ä½œæˆã™ã‚‹å ´åˆã¯ä¾‹ã«å€£ã„ãƒ­ãƒ¼ã‚«ãƒ«PCã§ä»¥ä¸‹ã‚’é †ã«å®Ÿè¡Œ
ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚‚åŸºæœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¾“ã†ã¹ã— 
```sh
$ PROJECT_ID=$(gcloud config list project --format "value(core.project)")
$ BUCKET_NAME=${PROJECT_ID}-aiplatform
$ REGION=us-central1
$ gsutil mb -l $REGION gs://$BUCKET_NAME
```
ãŸã ã—bucketã‚’ç™»éŒ²ã—ãªãã¦ã‚‚ã„ã„ã‚ˆã†ã«dockerã®setup.shã‚’å¤‰æ›´ã—ãŸã®ã§ä¸Šè¨˜è¨­å®šã¯ã—ãªãã¦ã‚‚ã„ã„ã‹ã‚‚ 
   
5. ãƒ­ãƒ¼ã‚«ãƒ«ã®shellã§èªè¨¼ã‚’è¡Œã†

ã“ã‚Œã§ã‚ã¨ã¯seedrl/gcp/train_<>.shã‚’å®Ÿè¡Œã™ã‚‹ã ã‘
(ä»¥ä¸‹ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã„ã‚‹ã®ã§ã“ã‚Œã§ã¯å‹•ã‹ãªã„)
ã€€ã€€
è©¦ã—ã«ã»ã¼ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šã§å›ã™  
(kaggleã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹checkoutã—ã¦ã„ã‚‹seedrlãƒ¬ãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨)  
ç¯€ç´„ã®ãŸã‚train_football_checkpoints.shã‚’å¤‰æ›´    
- maxTrialsã‚’1  
- total_environment_flamesã‚’10000  
- WORKERS 1
- ACTORS_PER_WORKERS 1

kuto branchã®custom checkpointsã‚’ä½¿ã†å ´åˆ.customã§wrapã—ã¦ã„ã‚‹ã®ã§
reward_experimentã¯scoringã®ã¿ã§è‰¯ã„(checkpointsãŒã‚ã£ã¦ã‚‚ãŠãã‚‰ãå•é¡Œã¯ãªã„)
åˆã‚ã¯buildã™ã‚‹ã®ã«20åˆ†ãã‚‰ã„ã‹ã‹ã‚‹   
ãã®ã‚ã¨Jobã‚’ç«‹ã¡ä¸Šã’ã‚‹ã®ã«10åˆ†ãã‚‰ã„   

ã‚¨ãƒ©ãƒ¼ãã®ï¼‘  
```
unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. <URL>
```
å¯¾å‡¦æ³•: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®URLå…ˆ:  
https://cloud.google.com/container-registry/docs/advanced-authentication?hl=ja ã‚’ç¢ºèª


ã‚¨ãƒ©ãƒ¼ãã®ï¼’
```
denied: Token exchange failed for project 'oceanic-hook-237214'. Please enable Google Container Registry API in Cloud Console at <URL>
```
å¯¾å‡¦æ³•: URLã«ã¨ã‚“ã§Google Container Registry APIã‚’æœ‰åŠ¹åŒ–

ã‚¨ãƒ©ãƒ¼ãã®ï¼“
```
ServiceException: 409 Bucket seed_rl already exists.
```
seed_rl/gcp/setup.sh
è‡ªåˆ†ã®bucketã‚’ä½¿ã†ã‚ˆã†ã«ä»¥ä¸‹ã®ã‚ˆã†ã«ä¿®æ­£
å‰ã®æ®µéšã§ã™ã§ã«ä½œæˆã—ã¦ã„ã‚‹å ´åˆã¯åŒã˜bucketè¨­å®šã¨ãªã‚‹ã‚ˆã†ã«
ä½œæˆã—ã¦ã„ãªã‘ã‚Œã°ä»¥ä¸‹ã®ã‚ˆã†ã«ã™ã‚‹
(kuto branchã§ã¯ä»¥ä¸‹ã¯ä¿®æ­£æ¸ˆã¿)

```sh
set -e
PROJECT_ID=$(gcloud config get-value project)
BUCKET_NAME=${PROJECT_ID}-aiplatform
REGION=us-central1
export IMAGE_URI=gcr.io/$PROJECT_ID/seed

start_training () {
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  $DIR/../docker/build.sh
  $DIR/../docker/push.sh
  # Create bucket if doesn't exist.
  # gsutil ls gs://seed_rl || gsutil mb gs://seed_rl
  gsutil ls gs://${BUCKET_NAME} || gsutil mb gs://${BUCKET_NAME}
  JOB_NAME="SEED_$(date +"%Y%m%d%H%M%S")"
  # Start training on AI platform.
  gcloud beta ai-platform jobs submit training ${JOB_NAME} \
    --project=${PROJECT_ID} \
    --job-dir gs://{$BUCKET_NAME}/${JOB_NAME} \
    --region ${REGION} \
    --config /tmp/config.yaml \
    --stream-logs -- --environment=${ENVIRONMENT} --agent=${AGENT} \
    --actors_per_worker=${ACTORS_PER_WORKER} --workers=${WORKERS} --
}
```
<!-- 
ã‚¨ãƒ©ãƒ¼ãã®ï¼”
å‹•ã„ãŸãŒlogã‚’ã¿ã‚‹ã¨errorãŒã§ã¦ã„ã‚‹  
ãŠãã‚‰ãä»¥ä¸‹ã®discussionã¨åŒæ§˜ã®ã‚¨ãƒ©ãƒ¼  
https://www.kaggle.com/c/google-football/discussion/192305

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãªã„ã¨ã®ã“ã¨ãªã®ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒåŸå› ã®å¯èƒ½æ€§ãŒé«˜ã„
train_football_checkpoints.shã®ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´
```
export WORKERS=4
export ACTORS_PER_WORKER=1
```

ã‚¨ãƒ©ãƒ¼ ãã®5
`Inference batch size is bigger than the number of actors.`
ä»¥ä¸‹ã®ã‚ˆã†ã«å¤‰æ›´  
```
export WORKERS=4
export ACTORS_PER_WORKER=8

- parameterName: inference_batch_size
  type: INTEGER
  minValue: 32
  maxValue: 32
``` -->
errorã½ã„ã‚‚ã®ãŒã§ã¦ã„ã‚‹ã‹ã²ã¨ã¾ãšã“ã‚Œã§GPUã§ã¯å›ã™ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸ    
hypertuneã¯å¤±æ•—ã¨ãªã£ã¦ã„ã‚‹ãŒlogã¨ãƒ¢ãƒ‡ãƒ«ã¯å–ã‚Œã¦ã„ã‚‹ã®ã§ç„¡è¦–ã§OK  

---

### seedrlã®çµæœã‚’tensorboardã«å‡ºåŠ›  
- bucketã®ãƒã‚¦ãƒ³ãƒˆ  
https://github.com/GoogleCloudPlatform/gcsfuse/   
- Cloud shellã‚’ä½¿ã£ã¦tensorboardã‚’åˆ©ç”¨  
https://gb-j.com/column/tensorboard/  

### Cloud shellã«ã‚ˆã‚‹tensorboardã®å¯è¦–åŒ–

```
$ gcsfuse oceanic-hook-237214-aiplatform ./tmp

$ tensorboard --logdir tmp/<logdir>
```

<!-- <div align="center"><img src="./img/015.png" title="result Îµ scheduling"></div>
<div align="center"><img src="./img/014.png" title="result Îµ scheduling"></div> -->

<div align="center"><img src="./img/017.png" title="result Îµ scheduling"></div>

å‡ºåŠ›ã•ã‚Œã‚‹log
- V(ä¾¡å€¤é–¢æ•°)
- learning_rate
- losses
- policy
- speed

ã•ã‚‰ã«æ¬²ã—ã„æƒ…å ±  
- å ±é…¬(çµ±è¨ˆé‡)
- difficulty
- checkpoints
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å¯è¦–åŒ–ã§ãã‚‹ï¼Ÿ

---
### [2020/10/24]

#### seedrlã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ç‚¹ã«ã¤ã„ã¦
ç¾çŠ¶ç‰¹ã«ã‚³ãƒŸãƒƒãƒˆã¨ã‹ã¯ã¾ã ã—ã¦ã„ãªã„

**NOTE** 
kuto_seed_rlã®å¤‰æ›´ç‚¹
- seed_rl/gcp/run.pyã®get_py_mainé–¢æ•°ã‚’adaptiveã«æ›¸ãæ›ãˆãŸ
- seed_rldocker/Dockerfile.footballã®dockerfileã§gfootballã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’2.7ã«å¤‰æ›´ã—ãŸ
- seed_rl/agents/vtrace/learner.pyã®minimizeé–¢æ•°ã®1è¡Œç›®ã§envã¸n_episodesã‚’æ¸¡ã™
```python
n_episodes = info_queue.size()
```
- adaptive_mainã«checkpointsã®wrapperã‚’è¿½åŠ   
Wrappã®é †ç•ªã®å•é¡Œã§ãã®ã¾ã¾wrappã™ã‚‹ã¨error  
CheckpointRewardWrapperã®rewardé–¢æ•°ã®I/Oã®typeã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§è§£æ±º   
 
 checkpointã®æ¸›è¡°
<div align="center"><img src="./img/016.png" title="result Îµ scheduling"></div>



### [2020/10/25]
ã‚„ã‚ŠãŸã„ã“ã¨5ç‚¹  
- raw return 1.1ä»¥ä¸Šã ã¨ï¼’ç‚¹ä»¥ä¸Šã¨ã„ã†ã“ã¨ã«ãªã‚‹ã®ã§ã¯å•é¡Œã®ç¢ºèª  
â†’ 3 episodesã®å¹³å‡ãªã®ã§å•é¡Œã¯ãªã„

- checkpointã‚’è¿½åŠ ã™ã‚‹ã¨errorã¨ãªã£ã¦ã„ã‚‹çŠ¶æ³  
â†’checkpointã‚’wrappã™ã‚‹ã¨ã“ã‚ã§å›ºã¾ã‚‹  
rainbowã§ã¯é©ç”¨ã§ãã‚‹  
é•ã„ã¨ã—ã¦ã¯difficultywrapperã¨SMMã‚µã‚¤ã‚ºã‚’å¤‰æ›´ã—ã¦ã„ã‚‹ã“ã¨  
```
to be float but double is provided
```  
checkpointå ±é…¬ã«epsilonã‚’ã‹ã‘ãŸã“ã¨ã§floatã®ä¸å‹•å°æ•°ç‚¹ãŒæœŸå¾…ã•ã‚Œã‚‹å€¤ã¨é•ã†å€¤ã«ãªã£ã¦ã—ã¾ã£ãŸï¼Ÿ
np.float32ã‚’checkpoint_rewardã«é©ç”¨ã—ã¦ã†ã¾ãã„ã£ãŸ!

- difficultyã€checkpointsã€å ±é…¬ã®æ¨ç§»ã®logåŒ–  
learnerã®compute_lossé–¢æ•°å†…ã«ä»¥ä¸‹ã‚’è¿½åŠ 
ã¾ãŸenvã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’é–¢æ•°å†…ã§ä½¿ç”¨ã—ãŸã„ã®ã§compute_lossã®å¼•æ•°ã«envã‚’è¿½åŠ 
```
logger.log(session, 'difficulty', env.difficulty)
logger.log(session, 'checkpoints_reward', env._checkpoints_reward)
```
tensorboardã‚’ç¢ºèªã™ã‚‹ã¨å¤‰åŒ–ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
<div align="center"><img src="./img/018.png" title="result Îµ scheduling"></div>
loggerã«æ¸¡ã—ãŸå€¤ãŒå®šæ•°æ‰±ã„ã¨ãªã£ã¦ã„ã‚‹ï¼Ÿ

- GCP tpuã®è¨­å®š
TPUã«ã¤ã„ã¦ 
https://cloud.google.com/ai-platform/training/docs/using-tpus?hl=ja#tpu-v3-beta

train_football_checkpoints.shå†…ã®config.yaml
```
tpuTfVersion: '2.2'
```

### [2020/10/26]
actorã®log
<div align="center"><img src="./img/019.png" title="result Îµ scheduling"></div>

actorå´ã§difficultyã¨ã‹checkpointã¯å–å¾—ã§ãã‚‹  
tensorboardã®logã«ã¤ã„ã¦  
https://www.tensorflow.org/api_docs/python/tf/summary

  
GCPã‚’åˆ©ç”¨ã™ã‚‹ä¸Šã§ã®æ•´ç†  
num_actor == num_env  
master == learner == tpu
worker == actor == cpu  

[2020/10/27]

remoteãƒ–ãƒ©ãƒ³ãƒã‚’å‰Šé™¤
```
git push --delete origin kuto
```

offsideã«ã¤ã„ã¦
https://github.com/google-research/football/blob/511e6412808bbc334cc29e78ba01919dd59f7e19/gfootball/env/football_env_test.py
ã“ã“ã§offsideã®testã‚³ãƒ¼ãƒ‰ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹
```
o[0]['right_team'][1][0] == 0
```
ä¸Šè¨˜ãŒ0ä»¥å¤–ã®æ™‚ã‚ªãƒ•ã‚µã‚¤ãƒ‰ã¨ãªã‚‹ï¼Ÿ
ãŸã ã—ã“ã‚Œã¯rawãƒ‡ãƒ¼ã‚¿ãªã‚“ã ã‘ã©SMMã§ã¯offsideã‚’åŒåˆ¤å®šã™ã‚Œã°ã„ã„ï¼Ÿ

game_mode
https://github.com/google-research/football/blob/10dae9d7039fc933128d52a92055ae3c76b70075/gfootball/doc/observation.md#observations--actions
6 = e_GameMode_Penalty
envã‚’unwrappedã—ã¦game_modeã¯å–å¾—å¯èƒ½

```
# add custom reward wrapper @kuto
class PenaltyRewardWrapper(gym.RewardWrapper):
  """A wrapper that adds a penalty reward."""

  def __init__(self, env):
    gym.RewardWrapper.__init__(self, env)
    self.penalty_reward = -0.1

  def reward(self, reward):
    reward = [reward]
    observation = self.env.unwrapped._env._observation()
    if observation is None:
      return reward

    assert len(reward) == len(observation)
    game_mode = observation[0]['game_mode']
    # game_mode: 6==penalty
    # https://github.com/google-research/football/blob/master/gfootball/doc/observation.md
    if game_mode == 6:
        reward += self.penalty_reward

    return reward[0]
```


```
masterType: n1-highmem-32
masterConfig:
  imageUri: ${IMAGE_URI}:${CONFIG}
  acceleratorConfig:
    count: 2
    type: NVIDIA_TESLA_P100

```
ã“ã“ã‚‰è¾ºã—ã°ã‚‰ãTPUã®ãƒã‚°ã¨ã‚Š  
ã—ã‹ã—è§£æ±ºã›ãš
higeponã•ã‚“ãŒdiscussionã§è³ªå•ã—ãŸãŒTPUã¯ç¾åœ¨ã¯ã†ã¾ãä½¿ãˆãªãã†?
GPUã«åˆ‡ã‚Šæ›¿ãˆã‚‹

### [2020/11/2]
ã‚„ã‚‹ã“ã¨
- Academyã‚’seedrlã§ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
- offsideã®reward
- penaltyãŒé‡è¤‡ã™ã‚‹ä¿®æ­£

### [2020/11/04]
academyã«ã‚ˆã‚‹éƒ¨åˆ†è¨“ç·´  
diffenceã®custom scenarioã‚’ä½œæˆã—220M stepsã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§è¿½åŠ å­¦ç¿’  
å¤§ä½“600 steps/s(2.1Msteps/hour)  
é€šå¸¸ã®scenarioã‚ˆã‚Šæ—©ã„ï¼Ÿ

<div align="center"><img src="./img/022.png" title="result Îµ scheduling"></div>
<div align="center"><img src="./img/023.png" title="result Îµ scheduling"></div>

academy agentæ„å¤–ã¨ã‚ˆããªã„
score 600ãã‚‰ã„
è©¦åˆã‚’è¦‹ã¦ã‚‚ç¢ºã‹ã«å¼±ããªã£ã¦ã„ã‚‹æ°—ãŒã™ã‚‹
- æ”»ã‚ã®å¼±ä½“åŒ–(ãƒ‘ã‚¹)
- å®ˆã‚Šã¯ã™ãæŠœã‘ã‚‰ã‚Œã¦ã—ã¾ã†  
  
åŸå› ã¯?  
- stackãƒŸã‚¹ã£ã¦ã„ã‚‹ï¼Ÿ
â†’ å•é¡ŒãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ãŸ
- ä¸Šæ›¸ãã—ãŸã“ã¨ãŒã¾ãšã„ï¼Ÿ

ãŠãã‚‰ãé•ã†ã‚·ãƒŠãƒªã‚ªã‚’è¿½åŠ å­¦ç¿’ã•ã›ãŸã“ã¨ã§agentãŒä»¥å‰ã®å†…å®¹ã‚’å¿˜ã‚Œã¦ã—ã¾ã£ãŸï¼Ÿ

### [2020/11/05]
academyãŒæœ‰åŠ¹ã«åƒãæ¡ä»¶ã‚’çŸ¥ã‚ŠãŸã„  

academyãŒã†ã¾ãæ©Ÿèƒ½ã—ãªã„åŸå› 
- å±€æ‰€çš„ãªå­¦ç¿’ã‚’ç¹°ã‚Šè¿”ã—è¡Œãªã£ãŸã“ã¨ã§å¿˜å´ãŒèµ·ããŸãŸã‚
- ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸Šã®äººæ•°ãŒå®Ÿéš›ã‚ˆã‚Šæ¥µç«¯ã«å°‘ãªã„ãŸã‚
- ã‚«ã‚¹ã‚¿ãƒ ã§å®Ÿè£…ã—ãŸscenarioã®è¨­å®šãŒã‚ˆããªã‹ã£ãŸãŸã‚
- checkpointã‚’ãªãã—ãŸãŸã‚

æ¤œè¨¼ã—ãŸã„ã“ã¨
- äººæ•°ã¯11äººã§è¡Œã†
- æ—¢å­˜ã®ã‚·ãƒŠãƒªã‚ªã§è¡Œã†
- ã‚·ãƒŠãƒªã‚ªã«æ‘‚å‹•ã‚’åŠ ãˆã‚‹
- checkpointã¯åŠ ãˆã‚‹


casestudy
- custom counter attack(random switch offence/diffence)
ã‚ã¡ã‚ƒãã¡ã‚ƒå¼±ããªã£ã¦ã„ã‚‹
- ã‚·ãƒŠãƒªã‚ªå¤‰ãˆãšã«ãã®ã¾ã¾å­¦ç¿’ã•ã›ã‚‹
å­¦ç¿’æ™‚é–“ãŒçŸ­ã„ã®ã§åˆ†ã‹ã‚‰ãªã„ãŒè©¦åˆã‚’è¦‹ã‚‹ã¨ã‚ˆãå‹•ã„ã¦ã„ã‚‹

- checkpointã‚’åŠ ãˆã¦ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒŠãƒªã‚ª11äººå­¦ç¿’
ç‚¹ã‚’å…¥ã‚Œã‚‰ã‚ŒãŸã¨ãã¯checkpointåæ˜ ã•ã‚Œãªã„(â†å‹˜é•ã„ã—ã¦ã„ãŸcheckpointã¯è²¬ã‚ã‚‹æ™‚ã ã‘)
ãªã‹ãªã‹ç‚¹æ•°ãŒæ±ºã¾ã‚‰ãªã„,ãªã‚“ã‹ä¸è‡ªç„¶ 
å‹•ç”»ã‚’è¦‹ã‚‹ã¨å‹•ãã¯ãã“ã¾ã§å¤‰ã˜ã‚ƒãªã„
â†’ scenarioã®è¨˜è¿°ã‚’11vs11ã«å¾“ã£ã¦æ›¸ã„ã¦ã¿ã‚‹(first_teamã¨second_teamãŒå…¥ã‚Œæ›¿ã‚ã‚‹ã‚„ã£ã¦ã„ã‚‹ã“ã¨ã¯åŒã˜)

- checkpointã®custom counter cenairo11äºº(ãã®ï¼’)
- ãƒ—ãƒ©ã‚¹ã®å ±é…¬ã—ã‹å‡ºã¦ã„ãªã„
- ãšã£ã¨æ”»ã‚ã®ã‚¿ãƒ¼ãƒ³ã«ãªã£ã¦ã„ã‚‹ï¼Ÿ
â†’ãƒœãƒ¼ãƒ«ã®ä½ç½®ãƒŸã‚¹ã£ã¦ãŸ

- checkpointã®custom counter cenairo11äºº(ãã®3)
ãã®ï¼‘ã¨åŒã˜çŠ¶æ³  
ã“ã‚Œã‚’é•·ã„å­¦ç¿’æ™‚é–“ã§æ§˜å­ã‚’è¦‹ã‚‹
ç¶šãã¯æ¬¡ã®æ—¥

self-playã ã¨å€å­¦ç¿’ã™ã‚‹ï¼Ÿ


### [2020/10/06]
custom counter attack scenario with checkpointã§5-10Mè¿½åŠ å­¦ç¿’
**æ¡ä»¶**
- äº¤äº’ã«ãƒãƒ¼ãƒ ã®å‘ãã‚’å¤‰ãˆã‚‹(æ”»ã‚ã¨å®ˆã‚ŠãŒå¤‰ã‚ã‚‹)
- ä½ç½®ã¯æ±ºå®šçš„
- äººæ•°11äºº
- counter attack scinario
- checkpointå ±é…¬ã‚ã‚Š

**çµæœ**  
- ä»¥å‰ã¨æ¯”ã¹ã¦å¤‰ãªæŒ™å‹•ã«ãªã£ã¦ã„ãªã„
- æ”»ã‚æ–¹ãŒå¤‰ã‚ã£ãŸ(ä»¥å‰ã¯ä¸­å¤®çªç ´ã€ä»Šå›ã¯ã‚µã‚¤ãƒ‰ã‹ã‚‰æ”»ã‚ã‚‹)
- æ”»ã‚æ–¹ã«å¤šæ§˜æ€§ãŒå°‘ãªã„ 
- ã‚¹ã‚³ã‚¢ã¯700ã¨å…ƒã€…ã‚ˆã‚Šè½ã¡ã¦ã„ã‚‹

### [2020/10/07]
ä¸€æ™‚çš„ã‹ã‚‚ã—ã‚Œãªã„ãŒã€hardã®subã®ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã¦ã‚‹ã¨760~950ã®ã‚ˆã†ã«subã«ã‚ˆã£ã¦çµæ§‹å¹…ãŒã‚ã‚‹ã€‚  
ã“ã‚Œã¯å˜ã«å­¦ç¿’ã•ã›ãŸã‚¹ãƒ†ãƒƒãƒ—æ•°(ã¤ã¾ã‚Šagentã®æ€§èƒ½)ã®å½±éŸ¿ã‹ã€   
ãã‚Œã¨ã‚‚è©¦åˆã®å¯¾æˆ¦ç›¸æ‰‹ã‚„ãã®çµæœã«ã‚ˆã‚‹é‹è¦ç´ ãªã®ã‹ã€‚
â†’ é‹è¦ç´ ã®æŒ¯ã‚Œå¹…ã‚’è¦‹ã‚‹ãŸã‚ã«åŒä¸€ã®subã‚’ï¼“å›è¡Œã„æŒ¯ã‚Œå¹…ã‚’ç¢ºèªã™ã‚‹    

åŒã˜ãƒ¢ãƒ‡ãƒ«ã§3ã¤æŠ•ç¨¿ã—ã¦ã¿ãŸçµæœ  
- ã‚¹ã‚³ã‚¢ã«ã¯ã–ã£ãã‚ŠÂ±100ã®èª¤å·®ãŒã‚ã‚‹
- é‹è¦ç´ ã«èµ·å› ã—ãŸã‚‚ã®ã§ã‚ã‚‹
- å¯¾æˆ¦ã‚’é‡ã­ã‚Œã°ã‚ã‚‹ç¨‹åº¦è¿‘ã„ã‚¹ã‚³ã‚¢ã«åæŸã—ã¦ã‚‚ã„ã„æ°—ãŒã™ã‚‹
- è‡ªä¿¡ãŒã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯è¤‡æ•°subã—ã¦ã‚¹ã‚³ã‚¢ã‚’è¦‹ãŸæ–¹ãŒè‰¯ã•ãã†
- å¯¾æˆ¦åºç›¤ã®ã‚¹ã‚³ã‚¢åˆ†æ•£ÏƒãŒå¤§ãã„æ™‚ã¯æ ¼ä¸Šã®æ•µã«å½“ãŸã‚Šã‚„ã™ãã€ãã“ã§å‹ã¤ã‹è² ã‘ã‚‹ã‹ã®é‹æ¬¡ç¬¬ã§ã‚¹ã‚³ã‚¢ã«å·®ãŒç”Ÿã¾ã‚Œã¦ã„ã‚‹?
- ã‚ã‚‹ç¨‹åº¦å¯¾æˆ¦ã‚’é‡ã­ã‚‹ã¨Ïƒã¯å°ã•ããªã‚‹ã®ã§æ ¼ä¸Šã®ç›¸æ‰‹ã¨æˆ¦ã†ãƒãƒ£ãƒ³ã‚¹ãŒå°‘ãªããªã‚Šã€åŒã˜ãƒ¢ãƒ‡ãƒ«ã§ã‚‚ã‚¹ã‚³ã‚¢ã¯åæŸã›ãšãã‚Œãã‚Œã®ã‚¹ã‚³ã‚¢å¸¯åŸŸã«è½ã¡ç€ã„ã¦ã„ã‚‹?


custom counter attack scenario with checkpointã§5-10Mè¿½åŠ å­¦ç¿’
**æ¡ä»¶**
- äº¤äº’ã«ãƒãƒ¼ãƒ ã®å‘ãã‚’å¤‰ãˆã‚‹(æ”»ã‚ã¨å®ˆã‚ŠãŒå¤‰ã‚ã‚‹)
- ä½ç½®ã¯ç¢ºç‡çš„
- äººæ•°11äºº
- counter attack scinario
- checkpointå ±é…¬ã‚ã‚Š

å¾Œã»ã©ä»¥ä¸‹ã‚’åŠ ãˆã‚‹
- hardã¨customã‚’äº¤äº’ã«è¡Œã†  

GCPè¨ˆç®—è³‡æº: n1-standard-96 * 2 + P100
è¨ˆç®—é€Ÿåº¦: 3K steps/s (10M steps/h)
è¨ˆç®—ã‚³ã‚¹ãƒˆ: $1.46(P100) + $0.0475*192(cpu) = $10.58/h

ã–ã£ãã‚Š1æ™‚é–“ã§10Mstepsã‚’1100å††ãã‚‰ã„

### [2020/10/08]
220M(hard) + 10M (counter attack with checkpoints)
<div align="center"><img src="./img/024.png" width=500 title="result Îµ scheduling"></div>
- ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒŠãƒªã‚ªã‚’è¦‹ã‚‹ã¨æ”»ã‚ãŒã‚ˆããªã„å°è±¡
- ç‰¹å¾´ã¨ã—ã¦ã¯æœ€åˆã¯ã‚†ã£ãã‚Šãƒ‰ãƒªãƒ–ãƒ«ã‚­ãƒ¼ãƒ‘ãƒ¼ã®ç›´å‰ã§ã‚·ãƒ¥ãƒ¼ãƒˆ
- ã‚µã‚¤ãƒ‰ã«é€²ã¿ã‚®ãƒªã‚®ãƒªã§ã‚´ãƒ¼ãƒ«ã«è¿‘ã¥ã„ã¦ã„ã
- é€šå¸¸ã®è©¦åˆã¯å•é¡Œãªã„æ„Ÿã˜ã ã£ãŸ
- ãŠãã‚‰ãcheckpointsã®å½±éŸ¿ã§ç‚¹ã‚’æ±ºã‚ãªãã¦ã‚‚å‰ã«é€²ã‚ã°è‰¯ã„ã¨å­¦ç¿’ã—ã¦ã„ã‚‹
- æ”»ã‚ã®æ™‚ã ã‘checkpointãŒåƒãã®ã§episode returnã¯0ä»¥ä¸Šã«ãªã‚‹

å¯¾ç­–
- without checkpointã§è¡Œã†(checkpointãŒæ€¥ã«ãªããªã‚‹ã¨å­¦ç¿’ãŒã†ã¾ãã„ã‹ãªããªã‚‹ã‹ã‚‚)
- diffenceã ã‘å­¦ç¿’ã•ã›ã‚‹ã€ãã†ã™ã‚Œã°checkpointã®å½±éŸ¿ã¯å—ã‘ãªã„(ãŸã ãã‚Œã ã¨ä¸€éƒ¨ã®ã‚·ãƒŠãƒªã‚ªã ã‘ã‚’å­¦ç¿’ã—ã¦ã—ã¾ã„æ±åŒ–æ€§èƒ½è½ã¡ã‚‹å•é¡Œã‚ã‚‹ã‹ã‚‚11äººã«ã—ãŸã®ã§å•é¡Œã¯ãªã„ï¼Ÿ)
- äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’checkpointæ¸›è¡°ã•ã›ã¦ãŠãã€è¿½åŠ å­¦ç¿’ã§ã¯checkpointã‚’ãªã—ã¨ã™ã‚‹(ã“ã‚ŒãŒç†æƒ³)

ã“ã‚Œã¾ã§ã®ã¾ã¨ã‚(220Mã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã‚·ãƒŠãƒªã‚ªã‚’å¤‰ãˆã¦è¿½åŠ å­¦ç¿’)

|ã‚·ãƒŠãƒªã‚ª|å¯¾æˆ¦ç›¸æ‰‹|checkpointså ±é…¬|é¸æ‰‹ã¨ãƒœãƒ¼ãƒ«ã®åˆæœŸä½ç½®|stepsæ•°|ã‚¹ã‚³ã‚¢(ç›®å®‰)|è©¦åˆã®æ§˜å­| 
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
|   counter attack(diffenceã®ã¿)  |  2 vs 4  |  ãªã—  |  æ±ºå®šçš„  | 7M |  500 | ã‹ãªã‚Šå¼±ããªã‚‹ |
|  counter attack(æ”»å®ˆã‚’äº¤äº’ã«)  |  11 vs 11  |  ã‚ã‚Š  |  æ±ºå®šçš„   | 5-10M | 800 | ã‚µã‚¤ãƒ‰ã‹ã‚‰ã®æ”»ã‚ã‚’è¦šãˆãŸãŒãƒ‘ã‚¿ãƒ¼ãƒ³åŒ–ã—ã¦ã‚‹  |
|  counter attack(æ”»å®ˆã‚’äº¤äº’ã«)   |  11 vs 11  |  ã‚ã‚Š  |  ç¢ºç‡çš„ |  10M  |  -  | æ”»ã‚ãŒå¼±ããªã£ã¦ã‚‹(checkpointã®ã›ã„ï¼Ÿ)ã€ãƒ‰ãƒªãƒ–ãƒ«ã«å¼±ã„  |
|  counter attack(æ”»å®ˆã‚’äº¤äº’ã«)   |  11 vs 11  |  ãªã—  |  ç¢ºç‡çš„  |  5M?  |  -  |  ã‹ãªã‚Šå¼±ããªã‚‹  |
|  counter attack(diffenceã®ã¿)   |  11 vs 11  | ã‚ã‚Š(diffenceã®ã¿ãªã®ã§å®Ÿè³ªãªã—)  |  ç¢ºç‡çš„  |  5M?  |  -  |  å‰ã®agentã«1ç‚¹å·®ã§è² ã‘ã‚‹/æ”»ã‚ãŒå¼±ããªã‚‹  |

æ•´ç†ã™ã‚‹ã¨
- checkpointsã‚’å®Œå…¨ã«ãªãã™ã¨ã‹ãªã‚Šå¼±ããªã‚‹(ã‚ã¡ã‚ƒãã¡ã‚ƒãªãƒ‘ã‚¹ã‚’ã—ãŸã‚Šã™ã‚‹ã‚ˆã†ã«ãªã‚‹)
- ä¸€ã¤ã®ã‚·ãƒŠãƒªã‚ªã ã‘ã‚’è¡Œã†ã¨ãã‚Œä»¥å¤–ã®éƒ¨åˆ†ãŒå¼±ããªã‚‹(diffenceã ã‘ã‚„ã‚‹ã¨æ”»ã‚ãŒå¼±ããªã‚‹)
- é€šå¸¸ã®ã‚·ãƒŠãƒªã‚ªã¯3000M stepsã§æœ€å¤§1.0ã®checkpoints rewardãŒå…¥ã‚‹ãŒã€ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒŠãƒªã‚ªã®å ´åˆ,100stepsç¨‹åº¦ã§æœ€å¤§1.0ã®checkpoints rewardãŒæ‰‹ã«å…¥ã‚‹
ãã®ãŸã‚å¾—ç‚¹ã®ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ãŒåƒãã«ããã€æ”»ã‚ãŒå¼±ããªã‚‹

ç¾çŠ¶ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒŠãƒªã‚ªã§ã‚„ã‚Œã‚‹ã“ã¨ã¨ã—ãŸã‚‰  
- checkpointsã‚’ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒŠãƒªã‚ªå†…ã§å¾ã€…ã«æ¸›è¡°ã•ã›ãªãŒã‚‰æ”»å®ˆäº¤äº’ã«å­¦ç¿’ã•ã›ã‚‹ã“ã¨
- é€šå¸¸ã®ã‚·ãƒŠãƒªã‚ªã¨ã‚«ã‚¹ã‚¿ãƒ ã‚·ãƒŠãƒªã‚ªã‚’äº¤äº’ã‚‚ã—ãã¯ç¢ºç‡çš„ã«åˆ‡ã‚Šæ›¿ãˆã¦å­¦ç¿’ã•ã›ã‚‹

### [2020/11/11]
è¿½åŠ å­¦ç¿’ã®éš›ã«checkpointã‚’æ¸›è¡°ã•ã›ã‚‹  
ä»¥ä¸‹æ¡ä»¶
|key|value|
| ---- | ---- |
|   checkpoint_num_episodes  | 10K(30Mstepsåˆ†) |
| scenario | 11_vs_11_hard_stochastic |
|  difficulty  |  1.0  | 
  

GCPã§ã¯æœ€åˆã‹ã‚‰checkpointã‚’æ¸›è¡°ã•ã›ã‚‹äºˆå®šãªã®ã«å¯¾ã—ã¦  
ã“ã¡ã‚‰ã¯å­¦ç¿’ãŒãã‚Œãªã‚Šã«å®‰å®šã—ã¦ããŸ220Mã‹ã‚‰æ¸›è¡°ã•ã›ã‚‹  
ã©ã¡ã‚‰ãŒè‰¯ã„ã‹æ¤œè¨¼

### [2020/11/12]
actorã¯åˆ†æ•£å­¦ç¿’ãªã®ã§checkpointã‚’resetã”ã¨ã«æ¸›è¡°ã•ã›ã‚‹å ´åˆã¯ãã®åˆ†æ•£ç’°å¢ƒå†…ã§0ã¾ã§æ¸›è¡°ã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹
ex)500Mstepsã§åˆ†æ•£ç’°å¢ƒãŒ100ã“ã‚ã‚‹å ´åˆå„ç’°å¢ƒã§ã¯5Mstepsã¨ãªã‚‹
  
### [2020/11/13]
difficultyã®æ¨ç§»ã«å¿œã˜ã¦checkpoint decayã‚’ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
ã“ã‚Œã«ã‚ˆã‚Šï¼’ã¤ã®æŒ™å‹•ã‚’ï¼‘ã¤ã«ã¾ã¨ã‚ã‚‰ã‚Œã‚‹ã®ã¨è² ã‘è¶Šã—ã¦ã„ã‚‹ã¨ãã«checkpointã‚’ä¸‹ã’ã‚‹ã¨ã„ã†ã“ã¨ã‚’é¿ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹  

checkpointã‚’è¿½åŠ ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expects arg[2] to be float but double is provided 
```
è§£æ±º
np.float32ã§éƒ¨åˆ†çš„ã«å›²ã‚€ã®ã§ã¯ãªãå¤‰æ•°ã«æ¸¡ã™å€¤å…¨ä½“ã‚’å›²ã‚€å¿…è¦ãŒã‚ã£ãŸ


### [2020/11/14]
checkpointã®ã‘ãŸæ•°ãŒé–“é•ã£ã¦ã„ãŸ
â†’ä¿®æ­£ã—å†åº¦å®Ÿè¡Œ  
    
difficultyã®ä¼¸ã³ãŒé…ã„å•é¡Œç™ºç”Ÿ
actorã‚’åˆ†æ•£ã•ã›ã¦ã„ã‚‹ã®ã§1actorã‚ãŸã‚Šã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ãŒå°‘ãªããªã£ã¦ã—ã¾ã†ã®ãŒåŸå› 
å¯¾ç­–ã¨ã—ã¦
- ï¼“è©¦åˆå¹³å‡1.1ä»¥ä¸Šâ†’ï¼‘è©¦åˆ1ä»¥ä¸Š
- difficultyæ›´æ–°å¹…0.001â†’0.05
- checkpointæ›´æ–°å¹… 0.00011 â†’ 0.0053
ã¨ã—ã¦æ—©ãæ¨ç§»ã™ã‚‹ã‚ˆã†ã«ã—ãŸ

### [2020/11/15]
100Mstepsæ™‚ç‚¹ã§difficulty=1.0ã«åˆ°é”  
saved_modelã‚’è½ã¨ã—ã¦submit  

æå‡ºã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã¨æ€ã‚ã‚Œã‚‹åŸå› 
- downloadã—ãŸã¨ãã¯assetsãŒãªã„  
- variableså†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½™åˆ†ãªéƒ¨åˆ†ãŒã‚ã‚‹
- localã§/kaggle_simlations/agent/ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã›ãšæå‡ºã—ãŸï¼Ÿ
â†“
- assetsã‚’æ‰‹å‹•ã§è¿½åŠ 
- variableså†…ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é€šå¸¸ã«åˆã‚ã›ã‚‹
- colabã§å®Ÿè¡Œ
ã“ã‚Œã§ã¨ã‚Šã‚ãˆãšå®Ÿè¡Œã§ããŸã€‚
subã—ã¦ãªã„ãŒcolab

```
 
```


4Msteps-diff/off å¼±ã„
å°‘é‡ã§ã‚‚checkpointãŒå¿…è¦ï¼Ÿ

### [2020/11/15]
500M çµ‚äº†
<div align="center"><img src="./img/025.png" width=500 title="result Îµ scheduling"></div>
<div align="center"><img src="./img/026.png" width=500 title="result Îµ scheduling"></div>
<div align="center"><img src="./img/027.png" width=500 title="result Îµ scheduling"></div>
<div align="center"><img src="./img/028.png" width=500 title="result Îµ scheduling"></div>
<div align="center"><img src="./img/029.png" width=500 title="result Îµ scheduling"></div>
<div align="center"><img src="./img/030.png" width=500 title="result Îµ scheduling"></div>

LB score 700~900
raw_returnã®å‰²ã‚Šã«scoreä¸ŠãŒã‚‰ãªã„
### [2020/11/15]
è¿½åŠ ã§100M  
å³è‚©ä¸ŠãŒã‚Šã§raw_return=2ã¾ã§ããŸ
scoreã¯ä½ã„ã¾ã¾  

### [2020/11/17]
### 900M
hardã«éå­¦ç¿’ã—ã¦ã„ãªã„ã‹ã‚’æ¤œè¨¼  
hardã¨ã®å¯¾æˆ¦(200Mãã‚‰ã„ã‹ã‚‰ãšã£ã¨hard)

<div align="center"><img src="./img/032.png" width=500 title="result Îµ scheduling"></div>
å…¬é–‹botã¨ã®å¯¾æˆ¦çµæœï¼ˆ20è©¦åˆåˆ†ï¼‰
<div align="center"><img src="./img/031.png" width=500 title="result Îµ scheduling"></div>
- 300Mã‹ã‚‰hardã«å¯¾ã—ã¦ã¯å¼·ããªã£ã¦ã„ã‚‹ãŒbotã«å¯¾ã—ã¦ã¯ã‚ã¾ã‚Šå¤‰ã‚ã‚‰ãªã„
- å¯¾botã§å­¦ç¿’ã‚’é€²ã‚ã¦ã„ãå¿…è¦ã‚ã‚Š  

æœ€é«˜ã‚¹ã‚³ã‚¢ã®kernelã®botã‚’å­¦ç¿’ã«æ··ãœã‚‹  
900M~1200M  

### [2020/11/25]
1285Mã¾ã§ããŸ  
localã§ã¯å¯¾botã«å‹ã¡è¶Šã—ãŸãŒLB  


### [2020/11/27]
### 1300M 
<div align="center"><img src="./img/034.png" width=500 title="result Îµ scheduling"></div>

localã®è©¦åˆå¯¾æˆ¦ã§ã¯hardã«ã‚‚botã«ã‚‚å‹åˆ©ã—ã¦ã„ã‚‹ãŒLB scoreãŒä¸‹ãŒã£ã¦ã„ã‚‹  
ä»–ã®ç³»çµ±ã®botã«è² ã‘ã¦ã„ã‚‹?    
ä»¥ä¸‹ã®RF botã¨å¯¾æˆ¦ã•ã›ãŸçµæœ   
https://www.kaggle.com/mlconsult/1149-ish-bot-rl-approximation   
ä½™è£•è² ã‘: vs random forest   
Totals 22:40 (2å‹7æ•—1å¼•ãåˆ†ã‘)   

ã“ã®botã‚‚å«ã‚ã¦å­¦ç¿’ã™ã‚‹å¿…è¦ãŒã‚ã‚‹  
ä»–ã«high scoreã®kernelã¯ãªã„ï¼Ÿ  
ä»¥ä¸‹ãŒ900å°ã®kernel  
https://www.kaggle.com/david1013/tunable-baseline-bot   
our:enermy = 44:37  
WIN:9 LOSE:7 TIE: 4  

https://www.kaggle.com/tomkelly0/lightgbm-10mins-940  
æœªæ¤œè¨¼

https://www.kaggle.com/illidan7/marauding-wingers-score-1041-1  
our:enermy = 32:35  
WIN:7 LOSE:7 TIE: 6  

äº’è§’ãªæ„Ÿã˜
ã²ã¨ã¾ãšã“ã„ã¤ã‚‰ã¯ç½®ã„ã¦ãŠã
randomforest botã‚’è¿½åŠ ã—ã¦50Mstepså®Ÿè¡Œã€€

### 1350M  
<div align="center"><img src="./img/035.png" width=500 title="result Îµ scheduling"></div>
RF-botã‚’è¿½åŠ ã—ãŸã“ã¨ã§scoreè½ã¡ãŸ(æƒ³å®šé€šã‚Šã§ã¯ã‚ã‚‹)  

### [2020/11/28]

bucketå†…ã®å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
```
gsutil cp -r gs://oceanic-hook-23~/SEED_hard_plus_bot_1500M/ ./
```
### 1550M  
hard:memory bot:rf-bot=1:1:5ã®æ¯”ã§å­¦ç¿’
å³è‚©ä¸ŠãŒã‚Š

<div align="center"><img src="./img/033.png" width=500 title="result Îµ scheduling"></div>


20è©¦åˆã®localã®çµæœ
| æ•µ | ã‚¹ã‚³ã‚¢ | å‹æ•— |
| --- | --- | --- |
| hard | 38:27 | 10å‹5æ•—5å¼•ãåˆ†ã‘ |
| memory-bot | 46:28 | 13å‹3æ•—4å¼•ãåˆ†ã‘ |
| RF-bot | 50:56 | 6å‹9æ•—5å¼•ãåˆ†ã‘ |

hard, memory-botã«ã¯å‹ã¡è¶Šã—ã¦ã„ã¦RF-botã«ã¯è² ã‘ã¦ã„ã‚‹ï¼Ÿ  
memory-botã®æ–¹ãŒå¼·ã„ã®ã§ã‚‚ã†å°‘ã—ãƒªã‚½ãƒ¼ã‚¹ã‚’memory-botã«ã•ãã¹ãï¼Ÿ  


### 1600M
hard:memory bot:rf-bot=1:2:5ã®æ¯”ã§å­¦ç¿’
<div align="center"><img src="./img/036.png" width=500 title="result Îµ scheduling"></div>
å³è‚©ä¸ŠãŒã‚Š


20è©¦åˆã®localã®çµæœ
| æ•µ | ã‚¹ã‚³ã‚¢ | å‹æ•— |
| --- | --- | --- |
| hard | 40:25 | 13å‹3æ•—4å¼•ãåˆ†ã‘ |
| memory-bot | 67:36 | 14å‹3æ•—3å¼•ãåˆ†ã‘ |
| RF-bot | 66:48 | 12å‹7æ•—1å¼•ãåˆ†ã‘ |


### 1650M
20è©¦åˆã®localã®çµæœ
<div align="center"><img src="./img/037.png" width=500 title="result Îµ scheduling"></div>
<div align="center"><img src="./img/038.png" width=500 title="result Îµ scheduling"></div>

40è©¦åˆ
| æ•µ | ã‚¹ã‚³ã‚¢ | å‹æ•— |
| --- | --- | --- |
| hard | 73:56 | 20å‹7æ•—13å¼•ãåˆ†ã‘ |
| memory-bot | 131:76 | 28å‹6æ•—6å¼•ãåˆ†ã‘ |
| RF-bot | 111:102 | 18å‹16æ•—6å¼•ãåˆ†ã‘ |


### [2020/11/30] æœ€çµ‚æ—¥

ä¸Šè¨˜ã®å›³ã‚’è¦‹ã‚‹ã¨returnã®æ³¢å½¢ã¯æŒ¯ã‚Œå¹…ãŒã‚ã‚‹  
ä¸Špeakã®ã¨ãã®ãƒ¢ãƒ‡ãƒ«ã¯å¼·ã„ã®ã§ã¯ï¼Ÿ
stepsã‚’åˆ»ã‚“ã§1620Mä»˜è¿‘ã§peakã‚’æ¢ç´¢ã™ã‚‹  
### 1620M
(10è©¦åˆ)
| æ•µ | ã‚¹ã‚³ã‚¢ | å‹æ•— |
| --- | --- | --- |
| hard | 30:14 | 6å‹0æ•—4å¼•ãåˆ†ã‘ |
| memory-bot | 27:16 | 6å‹3æ•—1å¼•ãåˆ†ã‘ |
| RF-bot | 40:27 | 5å‹5æ•—0å¼•ãåˆ†ã‘ |


### 1623M 
(20è©¦åˆ)
| æ•µ | ã‚¹ã‚³ã‚¢ | å‹æ•— |
| --- | --- | --- |
| hard | 66:24 | 16å‹1æ•—3å¼•ãåˆ†ã‘ |
| memory-bot | 74:38 | 13å‹5æ•—2å¼•ãåˆ†ã‘ |
| RF-bot | 70:51 | 13å‹5æ•—2å¼•ãåˆ†ã‘ |

### 1624M
(20è©¦åˆ)
| æ•µ | ã‚¹ã‚³ã‚¢ | å‹æ•— |
| --- | --- | --- |
| hard | 48:27 | 14å‹4æ•—2å¼•ãåˆ†ã‘ |
| memory-bot | 68:42 | 14å‹3æ•—3å¼•ãåˆ†ã‘ |
| RF-bot | 57:34 | 14å‹3æ•—3å¼•ãåˆ†ã‘ |

Although the score and ranking has not decided I'd like to share our kutopon solution.

First, thanks for everyone involved in organizing such a great competition. 
And also thanks my teammate @higepon .

Our approach is Reinforcement Learning by using [SEED RL](https://github.com/google-research/seed_rl).

## summary
- 1625M steps(about 541,666 games)
- adaptive difficulty & checkpoint reward decay
- train with multiple enemies(hard builtin AI & [memory pattern bot](https://www.kaggle.com/yegorbiryukov/gfootball-with-memory-patterns) & [random forest bot](https://www.kaggle.com/mlconsult/1149-ish-bot-rl-approximation) 

Thanks for Yegor Biryukov(@) and Ken Miller( @mlconsult )  to share your good notebook!

## steps
![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F2603247%2Fad29d4b1f848850cce5538bea01b3196%2F2020-11-30%2023.44.54.png?generation=1606915621593766&alt=media)
### 0~160M 
We trained our agent with adaptive difficulty & checkpoint decay.
builtin AI which was prepared in Gfootball environment can change the difficulty(range 0~1/easy:0.05/hard:0.95). And [this post](https://sites.google.com/view/rl-football/singleagent-team) say that CHECKPOINT reward is effective in the early stages but not effective well in further stages of training.
So We started to train with difficulty=0.05 and checkpoint reward=0.1 as the initial values and if the average rewards was above 1, we decided to raise the difficulty level by 0.05 and lower the checkpoint reward by 0.0053 in learner.
Our agent reached a difficulty of 1.0 and CHECKPOINT reward of 0.0 in 160M steps.

LB score is 700.

### 160M-900M
resume training with builtin AI bot(difficulty=1.0).
not improve the LB score although the reward return increased in training.

LB score is 800-900.

### 900M-1400M
playing against builtin AI bot and memory pattern bot.
Training rate is builtin AI : memory = 3:5(or 1:3)

LB score is 900-1000.

### 1400-1625M
playing against builtin AI bot and memory pattern bot and random forest bot.
Training rate is builtin AI : memory : random forest = 1:1:6(or 1:2:5)

LB score is 1000-1200.


validation game in final agent in 20 games.
| Opponent | Total score | Total results |
| --- | --- | --- |
| builtin AI | 48:27 | 14WIN 4LOSE 2TIE |
| memory pattern bot | 68:42 | 14WIN 3LOSE 3TIE |
| random forest bot | 57:34 | 14WIN 3LOSE 3TIE |

## not work
- training with custom scenario
- ensemble agent (decide the action by voting multiple RL agent)